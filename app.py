import io
import json
import re
import math
from typing import List, Dict, Tuple

import streamlit as st
from pypdf import PdfReader
from openai import OpenAI

# Optional (but recommended) deps:
# pip install numpy pandas
import numpy as np
import pandas as pd


# =========================================================
# 1) Page Configuration & Apple UX Style
# =========================================================
st.set_page_config(page_title="Report Mate", layout="centered")

st.markdown(
    """
    <style>
    @import url('https://fonts.googleapis.com/css2?family=SF+Pro+Display:wght@400;600;700&display=swap');

    html, body, [class*="css"] {
        font-family: 'SF Pro Display', -apple-system, BlinkMacSystemFont, sans-serif;
        background-color: #FBFBFD;
        color: #1D1D1F;
    }

    .report-card {
        background: white;
        padding: 30px;
        border-radius: 20px;
        box-shadow: 0 8px 30px rgba(0,0,0,0.04);
        margin-bottom: 25px;
        border: 1px solid #F2F2F7;
    }

    .main-header {
        font-size: 34px;
        font-weight: 700;
        letter-spacing: -0.5px;
        text-align: center;
        padding-top: 40px;
        margin-bottom: 5px;
    }

    .sub-header {
        font-size: 17px;
        color: #86868B;
        text-align: center;
        margin-bottom: 40px;
    }

    .stTextInput>div>div>input, .stFileUploader section {
        border-radius: 12px !important;
    }

    .stButton>button {
        width: 100%;
        border-radius: 12px;
        border: none;
        background-color: #0071E3;
        color: white;
        font-weight: 600;
        padding: 12px;
        transition: all 0.2s ease-in-out;
    }

    .stButton>button:hover {
        background-color: #0077ED;
        box-shadow: 0 4px 15px rgba(0,113,227,0.3);
    }

    div[data-testid="stPopover"] > button {
        background-color: #F5F5F7 !important;
        color: #0071E3 !important;
        border: 1px solid #D2D2D7 !important;
        border-radius: 8px !important;
        padding: 2px 8px !important;
        font-size: 12px !important;
        min-height: 24px !important;
        margin: 0 4px;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


# =========================================================
# 2) Session State Initialization
# =========================================================
def _init_state():
    defaults = {
        "result": None,
        "context_text": "",
        "last_inputs": {},
        "expansion_level": 0,
        # RAG store
        "rag_chunks": [],         # list[dict]: {text, file, page, chunk_id}
        "rag_embs": None,         # np.ndarray [N, D]
        "rag_ready": False,
        "rag_fingerprint": "",    # to know if we should rebuild
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v


_init_state()


# =========================================================
# 3) Sidebar (Settings + RAG + Length)
# =========================================================
with st.sidebar:
    st.markdown("### âš™ï¸ Settings")
    user_api_key = st.text_input("OpenAI API Key", type="password", placeholder="sk-...")
    model_name = st.text_input("Model", value="gpt-4o-mini")

    st.divider()
    st.markdown("### ğŸ§© Draft Length")
    base_paras = st.slider("ê¸°ë³¸ ë¬¸ë‹¨ ìˆ˜(ì†Œì ˆë‹¹)", min_value=2, max_value=3, value=2, step=1)
    expand_additional = st.slider("í™•ì¥ ì‹œ ì¶”ê°€ ë¬¸ë‹¨(ì†Œì ˆë‹¹)", min_value=1, max_value=2, value=1, step=1)
    min_chars_per_para = st.number_input("ë¬¸ë‹¨ ìµœì†Œ ê¸€ììˆ˜", min_value=120, max_value=600, value=200, step=10)

    st.divider()
    st.markdown("### ğŸ” RAG (ê²€ìƒ‰ ê¸°ë°˜ ì¸ìš© ê°•í™”)")
    use_rag = st.checkbox("RAG ì‚¬ìš© (PDFì—ì„œ ê´€ë ¨ ë¬¸ë‹¨ì„ ê²€ìƒ‰í•´ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±)", value=True)
    rag_top_k = st.slider("RAG Top-K (ì„¹ì…˜ë³„ ê°€ì ¸ì˜¬ ì¡°ê° ìˆ˜)", 3, 12, 6, 1)
    rag_max_pages_each = st.slider("PDFë‹¹ ìµœëŒ€ ì½ì„ í˜ì´ì§€ ìˆ˜", 5, 30, 12, 1)
    rag_chunk_chars = st.slider("Chunk í¬ê¸°(ë¬¸ì ìˆ˜)", 400, 1400, 900, 50)
    rag_overlap_chars = st.slider("Chunk overlap(ë¬¸ì ìˆ˜)", 0, 400, 150, 10)

    # Embedding model (stable default; change if you use a different one)
    embedding_model = st.text_input("Embedding model", value="text-embedding-3-small")

    st.divider()
    if st.button("ìƒˆ í”„ë¡œì íŠ¸ ì‹œì‘", use_container_width=True):
        st.session_state.clear()
        _init_state()
        st.rerun()


# =========================================================
# 4) Main UI
# =========================================================
st.markdown('<div class="main-header">Report Mate</div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">ì„ í–‰ì—°êµ¬ë¥¼ ë¶„ì„í•˜ì—¬ ë…¼ë¬¸ì˜ ë…¼ë¦¬ êµ¬ì¡°ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.</div>', unsafe_allow_html=True)

with st.container():
    st.markdown('<div class="report-card">', unsafe_allow_html=True)
    st.markdown("#### ğŸ–‹ï¸ ì—°êµ¬ ë§¥ë½ ì„¤ì •")
    topic = st.text_input("ì—°êµ¬ ì£¼ì œ", placeholder="ì˜ˆ: ìƒì„±í˜• AIê°€ ëŒ€í•™ìƒì˜ í•™ìˆ ì  ê¸€ì“°ê¸°ì— ë¯¸ì¹˜ëŠ” ì˜í–¥")

    col1, col2 = st.columns(2)
    with col1:
        purpose = st.text_input("ì—°êµ¬ ëª©ì ", placeholder="ì—°êµ¬ë¥¼ í†µí•´ ë¬´ì—‡ì„ ë°íˆê³  ì‹¶ë‚˜ìš”?")
    with col2:
        hypothesis = st.text_input("ì—°êµ¬ ê°€ì„¤", placeholder="ì˜ˆìƒë˜ëŠ” ê²°ë¡ ì€ ë¬´ì—‡ì¸ê°€ìš”?")

    uploaded_files = st.file_uploader(
        "ì„ í–‰ì—°êµ¬ PDF ì—…ë¡œë“œ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)", type=["pdf"], accept_multiple_files=True
    )
    st.markdown("</div>", unsafe_allow_html=True)


# =========================================================
# 5) Helpers: PDF extraction, chunking, embeddings, retrieval
# =========================================================
def get_pdf_pages_text(files, max_pages_each=10) -> List[Dict]:
    """
    Return list of dicts: {file, page, text}
    """
    pages = []
    for f in files:
        reader = PdfReader(io.BytesIO(f.getvalue()))
        file_name = f.name
        for i, page in enumerate(reader.pages[:max_pages_each]):
            content = page.extract_text() or ""
            content = content.strip()
            if content:
                pages.append({"file": file_name, "page": i + 1, "text": content})
    return pages


def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    """
    Simple char-based chunker with overlap.
    """
    if chunk_size <= 0:
        return [text]
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunks.append(text[start:end].strip())
        if end == n:
            break
        start = max(0, end - overlap)
    return [c for c in chunks if c]


def build_rag_store(files, api_key: str, embed_model: str, max_pages_each: int, chunk_size: int, overlap: int):
    """
    Build chunk list + embeddings and store in session_state.
    """
    client = OpenAI(api_key=api_key)

    pages = get_pdf_pages_text(files, max_pages_each=max_pages_each)
    chunks = []
    for p in pages:
        for j, ch in enumerate(chunk_text(p["text"], chunk_size, overlap)):
            chunks.append(
                {
                    "chunk_id": f'{p["file"]}::p{p["page"]}::c{j+1}',
                    "file": p["file"],
                    "page": p["page"],
                    "text": ch,
                }
            )

    if not chunks:
        raise ValueError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìŠ¤ìº”ë³¸/ì´ë¯¸ì§€ PDFì¼ ìˆ˜ ìˆì–´ìš”)")

    # Embed in batches
    texts = [c["text"] for c in chunks]
    embs = []
    batch_size = 128
    for i in range(0, len(texts), batch_size):
        batch = texts[i : i + batch_size]
        resp = client.embeddings.create(model=embed_model, input=batch)
        embs.extend([d.embedding for d in resp.data])

    embs_np = np.array(embs, dtype=np.float32)

    # Normalize for cosine similarity
    norms = np.linalg.norm(embs_np, axis=1, keepdims=True) + 1e-12
    embs_np = embs_np / norms

    st.session_state["rag_chunks"] = chunks
    st.session_state["rag_embs"] = embs_np
    st.session_state["rag_ready"] = True


def embed_query(text: str, api_key: str, embed_model: str) -> np.ndarray:
    client = OpenAI(api_key=api_key)
    resp = client.embeddings.create(model=embed_model, input=text)
    v = np.array(resp.data[0].embedding, dtype=np.float32)
    v = v / (np.linalg.norm(v) + 1e-12)
    return v


def rag_retrieve(query: str, top_k: int, api_key: str, embed_model: str) -> List[Dict]:
    """
    Return top_k chunks: [{file,page,text,score,chunk_id}, ...]
    """
    if not st.session_state.get("rag_ready", False):
        return []

    q = embed_query(query, api_key, embed_model)
    embs = st.session_state["rag_embs"]  # [N, D]
    sims = embs @ q  # cosine because normalized

    k = min(top_k, len(sims))
    idx = np.argpartition(-sims, k - 1)[:k]
    idx = idx[np.argsort(-sims[idx])]

    out = []
    chunks = st.session_state["rag_chunks"]
    for i in idx:
        c = chunks[int(i)]
        out.append(
            {
                "file": c["file"],
                "page": c["page"],
                "chunk_id": c["chunk_id"],
                "text": c["text"],
                "score": float(sims[int(i)]),
            }
        )
    return out


def build_section_context_rag(topic_: str, purpose_: str, hypothesis_: str, api_key: str, embed_model: str, top_k: int):
    """
    Build a compact, section-targeted context using RAG retrieval.
    """
    sections = {
        "ì„œë¡ ": ["ì—°êµ¬ ë°°ê²½", "ë¬¸ì œ ì œê¸°", "ì—°êµ¬ í•„ìš”ì„±", "ì—°êµ¬ ì§ˆë¬¸"],
        "ì´ë¡ ì  ë°°ê²½": ["í•µì‹¬ ê°œë… ì •ì˜", "ì„ í–‰ì—°êµ¬", "ì´ë¡ ", "ì—°êµ¬ ê³µë°±"],
        "ì—°êµ¬ë°©ë²•": ["ì—°êµ¬ ì„¤ê³„", "í‘œë³¸", "ì¸¡ì •", "ë³€ìˆ˜", "ë¶„ì„ ë°©ë²•", "íƒ€ë‹¹ë„"],
        "ê²°ë¡ ": ["ìš”ì•½", "í•¨ì˜", "í•œê³„", "í›„ì† ì—°êµ¬"],
    }

    context_parts = []
    for sec, hints in sections.items():
        q = f"{topic_} / {purpose_} / {hypothesis_} / {sec} / {' '.join(hints)}"
        hits = rag_retrieve(q, top_k=top_k, api_key=api_key, embed_model=embed_model)
        if hits:
            context_parts.append(f"\n=== SECTION CONTEXT: {sec} ===\n")
            for h in hits:
                # Include SOURCE/PAGE tag for the model to create [REF:íŒŒì¼ëª…,pìˆ«ì]
                context_parts.append(
                    f"[SOURCE: {h['file']}, PAGE: {h['page']}]\n{h['text']}\n"
                )

    return ("\n".join(context_parts)).strip()


def get_combined_text_with_meta(files, max_pages_each=10, max_chars=35000) -> str:
    """
    Fallback non-RAG context: first N pages combined.
    """
    text_data = ""
    for f in files:
        reader = PdfReader(io.BytesIO(f.getvalue()))
        file_name = f.name
        for i, page in enumerate(reader.pages[:max_pages_each]):
            content = page.extract_text() or ""
            content = content.strip()
            if content:
                text_data += f"\n[SOURCE: {file_name}, PAGE: {i+1}]\n{content}\n"
    return text_data[:max_chars]


def call_openai_json(api_key, model, system_msg, user_msg, temperature=0.45) -> Dict:
    client = OpenAI(api_key=api_key)
    resp = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}],
        response_format={"type": "json_object"},
        temperature=temperature,
    )
    return json.loads(resp.choices[0].message.content)


# =========================================================
# 6) Prompt builders (RAG-ready)
# =========================================================
def build_initial_prompt(topic_, purpose_, hypothesis_, context, base_paras_, min_chars_):
    system_msg = """
ë‹¹ì‹ ì€ ì„ì‚¬í•™ìœ„ ë…¼ë¬¸ì„ ë‹¤ìˆ˜ ì§€ë„í•œ ì „ë¬¸ í•™ìˆ  ì—ë””í„°ì…ë‹ˆë‹¤.
ì œê³µëœ ìë£Œì— ê·¼ê±°í•´ ì—„ë°€í•œ í•™ìˆ  ë¬¸ì²´(ì„ì‚¬ ë…¼ë¬¸ ìˆ˜ì¤€)ë¡œ ì„œìˆ í•˜ê³ , ì£¼ì¥-ê·¼ê±°-ë¹„íŒì  ë…¼ì˜-ì—°êµ¬ ê³µë°±/ê¸°ì—¬ì˜ ì—°ê²°ì„ ë¶„ëª…íˆ í•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ ì§€ì •í•œ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    user_msg = f"""
ì£¼ì œ: {topic_}
ëª©ì : {purpose_}
ê°€ì„¤: {hypothesis_}

[ìë£Œ ì›ë¬¸]
{context}

[ì¶œë ¥ ì–¸ì–´]
- í•œêµ­ì–´

[í•µì‹¬ ìš”êµ¬ì‚¬í•­]
1) detailed_outline (ê°„ê²°): ê° ì„¹ì…˜(ì„œë¡ /ì´ë¡ ì  ë°°ê²½/ì—°êµ¬ë°©ë²•/ê²°ë¡ )ë‹¹ 6~10ë¬¸ì¥ ì´ë‚´ë¡œ 'ì „ê°œ ì „ëµ' ìš”ì•½.
2) interactive_draft (ì „ë¬¸ì /ì¶©ë¶„í•œ ë¶„ëŸ‰):
   - ê° ì„¹ì…˜ì„ ì†Œì ˆë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±: ì˜ˆ) ì„œë¡ ì€ 1.1~1.4, ì´ë¡ ì  ë°°ê²½ì€ 2.1~2.4, ì—°êµ¬ë°©ë²•ì€ 3.1~3.5, ê²°ë¡ ì€ 4.1~4.4 (í•„ìš” ì‹œ ì¡°ì • ê°€ëŠ¥).
   - ê° ì†Œì ˆì€ ìµœì†Œ {base_paras_}ê°œ ë¬¸ë‹¨.
   - ê° ë¬¸ë‹¨ì€ ìµœì†Œ {min_chars_}ì ì´ìƒ.
   - ê° ë¬¸ë‹¨ì—ëŠ” ìµœì†Œ 1ê°œì˜ ì¸ìš© íƒœê·¸ [REF:íŒŒì¼ëª…,pìˆ«ì] í¬í•¨(ê°€ëŠ¥í•˜ë©´ 2ê°œ).
   - ë¬¸ë‹¨ ì „ê°œì— (ì£¼ì¥/ìš”ì§€ â†’ ê·¼ê±° ì—°ê²° â†’ ë¹„íŒì  ë…¼ì˜/í•œê³„ â†’ ì—°êµ¬ ê³µë°± ë° ë³¸ ì—°êµ¬ì˜ ìœ„ì¹˜í™”) ìš”ì†Œë¥¼ ê· í˜• ìˆê²Œ í¬í•¨.
3) source_map:
   - ê° [REF:...] íƒœê·¸ì— ëŒ€ì‘í•˜ëŠ” 'í•´ë‹¹ í˜ì´ì§€ì˜ í•µì‹¬ ê·¼ê±° ìš”ì•½'ì„ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±.
4) REF ê·œì¹™:
   - REF íƒœê·¸ í¬ë§·ì€ ë°˜ë“œì‹œ ì •í™•íˆ [REF:íŒŒì¼ëª…,pìˆ«ì]
   - íŒŒì¼ëª…ì€ SOURCEì— ë‚˜ì˜¨ íŒŒì¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ê²ƒ.
   - í˜ì´ì§€ ìˆ«ìëŠ” SOURCEì˜ PAGE ê°’ì„ ê·¼ê±°ë¡œ í•  ê²ƒ.

[JSON ìŠ¤í‚¤ë§ˆ - ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¡œë§Œ ì¶œë ¥]
{{
  "detailed_outline": {{
    "ì„œë¡ ": "...",
    "ì´ë¡ ì  ë°°ê²½": "...",
    "ì—°êµ¬ë°©ë²•": "...",
    "ê²°ë¡ ": "..."
  }},
  "interactive_draft": {{
    "ì„œë¡ ": "...",
    "ì´ë¡ ì  ë°°ê²½": "...",
    "ì—°êµ¬ë°©ë²•": "...",
    "ê²°ë¡ ": "..."
  }},
  "source_map": {{
    "[REF:íŒŒì¼ëª…,pìˆ«ì]": "ì´ REFê°€ ì§€ì§€í•˜ëŠ” í•µì‹¬ ê·¼ê±°(í•´ë‹¹ í˜ì´ì§€ ë‚´ìš©) ìš”ì•½"
  }}
}}
"""
    return system_msg, user_msg


def build_expand_prompt(topic_, purpose_, hypothesis_, context, current_result, add_paras_, min_chars_):
    system_msg = """
ë‹¹ì‹ ì€ ì„ì‚¬í•™ìœ„ ë…¼ë¬¸ì„ ë‹¤ìˆ˜ ì§€ë„í•œ ì „ë¬¸ í•™ìˆ  ì—ë””í„°ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ê¸°ì¡´ ì´ˆì•ˆì„ ë” ì „ë¬¸ì ì´ê³  ë” ê¸¸ê²Œ 'í™•ì¥'í•©ë‹ˆë‹¤. ê·¼ê±°(REF) ë°€ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë…¼ë¦¬ì  ì—°ê²°ê³¼ ë¹„íŒì  ë…¼ì˜ë¥¼ ê°•í™”í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ì§€ì •í•œ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
    user_msg = f"""
ì£¼ì œ: {topic_}
ëª©ì : {purpose_}
ê°€ì„¤: {hypothesis_}

[ìë£Œ ì›ë¬¸]
{context}

[ê¸°ì¡´ ê²°ê³¼(JSON)]
{json.dumps(current_result, ensure_ascii=False)}

[í™•ì¥ ìš”êµ¬ì‚¬í•­]
- interactive_draftë§Œ 'ë” ê¸¸ê²Œ' í™•ì¥(ì†Œì ˆë³„ ë¬¸ë‹¨ ì¶”ê°€).
- ê° ì†Œì ˆ(ì˜ˆ: 1.1, 1.2...)ë§ˆë‹¤ ë¬¸ë‹¨ì„ ì¶”ê°€ë¡œ {add_paras_}ê°œì”© ë” ì‘ì„±.
- ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” ê° ë¬¸ë‹¨ì€ ìµœì†Œ {min_chars_}ì ì´ìƒ.
- ìƒˆ ë¬¸ë‹¨ë§ˆë‹¤ ìµœì†Œ 1ê°œì˜ [REF:íŒŒì¼ëª…,pìˆ«ì] í¬í•¨(ê°€ëŠ¥í•˜ë©´ 2ê°œ).
- source_mapì—ëŠ” ìƒˆë¡œ ë“±ì¥í•œ REFê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì¶”ê°€í•˜ê³ , ê¸°ì¡´ REF ë§¤í•‘ë„ ìœ ì§€/ë³´ê°•.
- ê¸°ì¡´ ì„œìˆ ê³¼ ëª¨ìˆœë˜ì§€ ì•Šê²Œ í•˜ë˜, í•™ìˆ ì  ì—°ê²°ì–´/ê°œë… ì •êµí™”/í•œê³„ ë° ì—°êµ¬ê³µë°±ì„ ë” ëª…í™•íˆ.

[JSON ìŠ¤í‚¤ë§ˆ - ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¡œë§Œ ì¶œë ¥]
{{
  "detailed_outline": {{
    "ì„œë¡ ": "...",
    "ì´ë¡ ì  ë°°ê²½": "...",
    "ì—°êµ¬ë°©ë²•": "...",
    "ê²°ë¡ ": "..."
  }},
  "interactive_draft": {{
    "ì„œë¡ ": "...",
    "ì´ë¡ ì  ë°°ê²½": "...",
    "ì—°êµ¬ë°©ë²•": "...",
    "ê²°ë¡ ": "..."
  }},
  "source_map": {{
    "[REF:íŒŒì¼ëª…,pìˆ«ì]": "ì´ REFê°€ ì§€ì§€í•˜ëŠ” í•µì‹¬ ê·¼ê±°(í•´ë‹¹ í˜ì´ì§€ ë‚´ìš©) ìš”ì•½"
  }}
}}
"""
    return system_msg, user_msg


# =========================================================
# 7) Rendering + Reference export helpers
# =========================================================
def render_text_with_ref_popovers(text: str, source_map: Dict[str, str]):
    parts = re.split(r"(\[REF:[^\]]+\])", text)
    buffer = ""
    for part in parts:
        if part.startswith("[REF:"):
            if buffer.strip():
                st.markdown(buffer)
            buffer = ""
            ref_info = source_map.get(part, "ìƒì„¸ ì¶œì²˜ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            with st.popover(f"ğŸ“ {part}"):
                st.markdown(f"**ìƒì„¸ ê·¼ê±°:**\n\n{ref_info}")
        else:
            buffer += part
    if buffer.strip():
        st.markdown(buffer)


def extract_refs_from_draft(draft: Dict[str, str]) -> List[str]:
    all_text = "\n".join(draft.values())
    refs = re.findall(r"\[REF:[^\]]+\]", all_text)
    # preserve order, unique
    seen = set()
    out = []
    for r in refs:
        if r not in seen:
            seen.add(r)
            out.append(r)
    return out


def parse_ref_tag(ref_tag: str) -> Tuple[str, int]:
    # [REF:íŒŒì¼ëª…,pìˆ«ì]
    m = re.match(r"\[REF:(.+?),p(\d+)\]", ref_tag)
    if not m:
        return ref_tag, -1
    return m.group(1), int(m.group(2))


def build_reference_table(source_map: Dict[str, str], used_refs: List[str]) -> pd.DataFrame:
    rows = []
    for ref in used_refs:
        file_name, page = parse_ref_tag(ref)
        rows.append(
            {
                "ref_tag": ref,
                "file": file_name,
                "page": page,
                "evidence_summary": source_map.get(ref, ""),
            }
        )
    return pd.DataFrame(rows)


def export_markdown_with_footnotes(draft: Dict[str, str], source_map: Dict[str, str]) -> str:
    """
    Convert [REF:...] into footnote markers [^n] and append footnotes list.
    """
    used_refs = extract_refs_from_draft(draft)
    ref_to_idx = {r: i + 1 for i, r in enumerate(used_refs)}

    md_parts = ["# Report Mate Draft\n"]
    for sec, txt in draft.items():
        md_parts.append(f"\n## {sec}\n")
        # Replace refs with footnotes
        def _repl(m):
            r = m.group(0)
            idx = ref_to_idx.get(r, None)
            return f"[^{idx}]" if idx is not None else r

        converted = re.sub(r"\[REF:[^\]]+\]", _repl, txt)
        md_parts.append(converted.strip() + "\n")

    md_parts.append("\n---\n\n## References (Footnotes)\n")
    for r in used_refs:
        idx = ref_to_idx[r]
        # Keep the original tag in the footnote for traceability
        md_parts.append(f"[^{idx}]: {r} â€” {source_map.get(r, 'ìƒì„¸ ê·¼ê±° ì—†ìŒ')}\n")
    return "\n".join(md_parts).strip()


# =========================================================
# 8) Generate / Expand Actions (with RAG)
# =========================================================
colA, colB = st.columns(2)
with colA:
    generate_clicked = st.button("ğŸš€ ë¶„ì„ ë° ìƒì„¸ ì´ˆì•ˆ ìƒì„±", type="primary")
with colB:
    expand_clicked = st.button("â• ì´ˆì•ˆ í™•ì¥(ì¶”ê°€ ì‘ì„±)", type="secondary", disabled=(st.session_state["result"] is None))

def compute_files_fingerprint(files) -> str:
    # cheap fingerprint: file names + sizes
    if not files:
        return ""
    parts = []
    for f in files:
        try:
            parts.append(f"{f.name}:{len(f.getvalue())}")
        except Exception:
            parts.append(f"{f.name}:?")
    return "|".join(parts)

if generate_clicked:
    if not user_api_key or not uploaded_files or not topic:
        st.warning("API í‚¤, ì£¼ì œ, ê·¸ë¦¬ê³  íŒŒì¼ì„ ëª¨ë‘ ì…ë ¥í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì„ í–‰ì—°êµ¬ë¥¼ ë¶„ì„í•˜ë©° ìƒì„¸ ì„¤ê³„ì•ˆ/ì´ˆì•ˆì„ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                st.session_state["expansion_level"] = 0

                # ---- Build context (RAG or fallback) ----
                context = ""
                files_fp = compute_files_fingerprint(uploaded_files)
                rag_fp = f"{files_fp}::{rag_max_pages_each}::{rag_chunk_chars}::{rag_overlap_chars}::{embedding_model}"

                if use_rag:
                    # Build RAG store if not ready or fingerprint changed
                    if (not st.session_state.get("rag_ready", False)) or (st.session_state.get("rag_fingerprint", "") != rag_fp):
                        st.session_state["rag_ready"] = False
                        st.session_state["rag_fingerprint"] = rag_fp
                        build_rag_store(
                            files=uploaded_files,
                            api_key=user_api_key,
                            embed_model=embedding_model,
                            max_pages_each=rag_max_pages_each,
                            chunk_size=rag_chunk_chars,
                            overlap=rag_overlap_chars,
                        )
                    context = build_section_context_rag(
                        topic_=topic,
                        purpose_=purpose,
                        hypothesis_=hypothesis,
                        api_key=user_api_key,
                        embed_model=embedding_model,
                        top_k=rag_top_k,
                    )

                    if not context.strip():
                        # fallback if retrieval context is empty for some reason
                        context = get_combined_text_with_meta(uploaded_files, max_pages_each=rag_max_pages_each)
                else:
                    context = get_combined_text_with_meta(uploaded_files, max_pages_each=rag_max_pages_each)

                st.session_state["context_text"] = context
                st.session_state["last_inputs"] = {
                    "topic": topic,
                    "purpose": purpose,
                    "hypothesis": hypothesis,
                    "base_paras": base_paras,
                    "min_chars_per_para": min_chars_per_para,
                    "model_name": model_name,
                    "use_rag": use_rag,
                    "rag_top_k": rag_top_k,
                    "rag_max_pages_each": rag_max_pages_each,
                    "rag_chunk_chars": rag_chunk_chars,
                    "rag_overlap_chars": rag_overlap_chars,
                    "embedding_model": embedding_model,
                }

                system_msg, user_msg = build_initial_prompt(
                    topic_=topic,
                    purpose_=purpose,
                    hypothesis_=hypothesis,
                    context=context,
                    base_paras_=base_paras,
                    min_chars_=min_chars_per_para,
                )
                st.session_state["result"] = call_openai_json(
                    api_key=user_api_key,
                    model=model_name,
                    system_msg=system_msg,
                    user_msg=user_msg,
                    temperature=0.45,
                )

            except Exception as e:
                st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if expand_clicked:
    if not user_api_key:
        st.warning("ë¨¼ì € OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif st.session_state["result"] is None:
        st.warning("ë¨¼ì € ì´ˆì•ˆì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì´ˆì•ˆì„ ë” ì „ë¬¸ì ìœ¼ë¡œ í™•ì¥ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                last = st.session_state.get("last_inputs", {})
                topic0 = last.get("topic", topic)
                purpose0 = last.get("purpose", purpose)
                hypothesis0 = last.get("hypothesis", hypothesis)
                model0 = last.get("model_name", model_name)

                # Rebuild context (keeps RAG settings consistent with last run)
                context = st.session_state.get("context_text", "")
                # If user changed PDFs after initial generation, context might be stale;
                # but we keep last run consistency intentionally.
                # If you'd rather rebuild context on expand, set context again here.

                system_msg, user_msg = build_expand_prompt(
                    topic_=topic0,
                    purpose_=purpose0,
                    hypothesis_=hypothesis0,
                    context=context,
                    current_result=st.session_state["result"],
                    add_paras_=expand_additional,
                    min_chars_=min_chars_per_para,
                )
                st.session_state["result"] = call_openai_json(
                    api_key=user_api_key,
                    model=model0,
                    system_msg=system_msg,
                    user_msg=user_msg,
                    temperature=0.5,
                )
                st.session_state["expansion_level"] += 1

            except Exception as e:
                st.error(f"í™•ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# =========================================================
# 9) Result Display + Reference Export
# =========================================================
if st.session_state["result"]:
    res = st.session_state["result"]
    st.markdown("---")

    if st.session_state.get("expansion_level", 0) > 0:
        st.info(f"ì´ˆì•ˆì´ {st.session_state['expansion_level']}íšŒ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    tab1, tab2, tab3 = st.tabs(["ğŸ“‹ ìƒì„¸ ì„¤ê³„ ê°œìš”(ê°„ê²°)", "âœï¸ ê°ì£¼ í¬í•¨ ì´ˆì•ˆ(ì „ë¬¸ì )", "ğŸ“¤ Reference Export"])

    with tab1:
        for section, detail in res.get("detailed_outline", {}).items():
            st.markdown(
                f"""
                <div class="report-card">
                    <div style="color: #0071E3; font-weight: 700; font-size: 18px; margin-bottom: 12px;">{section}</div>
                    <div style="color: #424245; line-height: 1.7; font-size: 15px;">{detail}</div>
                </div>
                """,
                unsafe_allow_html=True,
            )

    with tab2:
        source_map = res.get("source_map", {})
        draft = res.get("interactive_draft", {})

        for section, text in draft.items():
            st.markdown(f"### {section}")
            render_text_with_ref_popovers(text, source_map)
            st.divider()

    with tab3:
        source_map = res.get("source_map", {})
        draft = res.get("interactive_draft", {})

        used_refs = extract_refs_from_draft(draft)
        ref_df = build_reference_table(source_map, used_refs)

        st.markdown("#### ğŸ”— ì‚¬ìš©ëœ REF ëª©ë¡")
        st.caption("ì´ˆì•ˆì— ì‹¤ì œë¡œ ë“±ì¥í•œ [REF:...]ë§Œ ì¶”ì¶œí•´ ë‚´ë³´ëƒ…ë‹ˆë‹¤.")
        st.dataframe(ref_df, use_container_width=True, hide_index=True)

        # --- Export: CSV / JSON / Markdown with footnotes ---
        csv_bytes = ref_df.to_csv(index=False).encode("utf-8-sig")
        json_bytes = json.dumps(
            {"used_refs": used_refs, "source_map_used": {r: source_map.get(r, "") for r in used_refs}},
            ensure_ascii=False,
            indent=2,
        ).encode("utf-8")

        md_text = export_markdown_with_footnotes(draft, source_map).encode("utf-8")

        colx, coly, colz = st.columns(3)
        with colx:
            st.download_button(
                "â¬‡ï¸ REF Table (CSV)",
                data=csv_bytes,
                file_name="references_table.csv",
                mime="text/csv",
                use_container_width=True,
            )
        with coly:
            st.download_button(
                "â¬‡ï¸ REF Map (JSON)",
                data=json_bytes,
                file_name="references_map.json",
                mime="application/json",
                use_container_width=True,
            )
        with colz:
            st.download_button(
                "â¬‡ï¸ Draft + Footnotes (MD)",
                data=md_text,
                file_name="draft_with_footnotes.md",
                mime="text/markdown",
                use_container_width=True,
            )

        st.markdown("---")
        st.markdown("#### âœ… Export íŒ")
        st.markdown(
            """
- **CSV**: ë…¼ë¬¸/í˜ì´ì§€ë³„ ê·¼ê±°ë¥¼ ì •ë¦¬í•´ì„œ ê²€í† Â·ë³´ì™„í•  ë•Œ ìœ ìš©
- **JSON**: ë‹¤ë¥¸ ì‹œìŠ¤í…œ(Word/Notion ë³€í™˜, DB ì €ì¥, í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸)ì— ì—°ê²°í•˜ê¸° ì¢‹ìŒ
- **MD**: [REF]ë¥¼ ê°ì£¼ë¡œ ë°”ê¿”ì„œ ë¬¸ì„œ í˜•íƒœë¡œ ë°”ë¡œ í™œìš© ê°€ëŠ¥
"""
        )

else:
    st.markdown(
        "<br><br><p style='text-align: center; color: #BFBFC3;'>ì„ í–‰ì—°êµ¬ë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•˜ì—¬ ë…¼ë¬¸ ì´ˆì•ˆì„ í™•ì¸í•˜ì„¸ìš”.</p>",
        unsafe_allow_html=True,
    )

st.markdown(
    '<p style="text-align: center; color: #D2D2D7; font-size: 12px; margin-top: 50px;">Â© 2026 Report Mate. Designed for Academic Excellence.</p>',
    unsafe_allow_html=True,
)
